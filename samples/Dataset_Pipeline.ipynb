{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pipeline\n",
    "\n",
    "This code takes a financial market data file and runs it through a processing pipeline. The following operations are carried out :\n",
    "\n",
    "- Localise the time data to market time\n",
    "- Merge with existing dataset based on datetime\n",
    "- Crop data\n",
    "- Resample data\n",
    "- Normalise data \n",
    "- Encode category data\n",
    "- Concatenate additional columns\n",
    "- Shuffle data\n",
    "- Split into Traning/Validation/Test set\n",
    "- Visualise data\n",
    "- Save the results to HDF5\n",
    "- Save the results to CSV\n",
    "\n",
    "The pipeline functions are included below, but are also part of the quantutils library provided by this github repository where this notebook lives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note : This notebook should be run on your local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## Merge data\n",
    "##\n",
    "\n",
    "def merge(newData, existingData):\n",
    "    print \"Merging data...\"\n",
    "    return existingData.combine_first(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Shuffle data\n",
    "##\n",
    "def shuffle(data):\n",
    "    return data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Concatenate Columns\n",
    "##\n",
    "\n",
    "def concat(data1, data2):\n",
    "    print(\"Concatenating features %s with classifications %s\" % (data1.shape, data2.shape))\n",
    "    return pandas.DataFrame(numpy.concatenate([data1.values, data2.values], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Crop\n",
    "##\n",
    "\n",
    "def cropDate(data, start, end):\n",
    "    return data[start:end]\n",
    "\n",
    "def cropTime(data, start, end):\n",
    "    return data.between_time(start, end, include_start=True, include_end=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Resample\n",
    "##\n",
    "\n",
    "def resample(data, sample_unit):\n",
    "    print(\"Resampling to %s periods\" % sample_unit)\n",
    "    order = data.columns\n",
    "    return data.resample(sample_unit).agg({'Open': 'first', 'High': lambda x : x.max(skipna=False), 'Low': lambda x : x.min(skipna=False),'Close': 'last'})[order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Remove Missing Data (NaN)\n",
    "##\n",
    "\n",
    "def removeNaNs(data):\n",
    "    return data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Convert to Feature Matrix\n",
    "##\n",
    "\n",
    "def toFeatureSet(data, feature_periods):\n",
    "    n = data.values.shape[1] * feature_periods\n",
    "    return pandas.DataFrame(reshape(data, n))\n",
    "\n",
    "def reshape(ts, n):\n",
    "    return numpy.reshape(ts.values[0:ts.values.size / n * n / ts.values.shape[1]], (ts.values.size / n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Encode classification\n",
    "##\n",
    "\n",
    "def encode(data, encoding):\n",
    "    nanIndex = data.isnull().any(axis=1)\n",
    "    if (encoding == \"binary\"):\n",
    "        df = pandas.DataFrame((data.values[:,-1] > data.values[:,0]).astype(float))\n",
    "    if (encoding == \"one-hot\"):\n",
    "        df = pandas.DataFrame(numpy.column_stack\n",
    "                                ([(data.values[:,-1] > data.values[:,0]).astype(float), \n",
    "                                  (data.values[:,0] > data.values[:,-1]).astype(float)])\n",
    "                                )\n",
    "    df[nanIndex] = numpy.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Convert to local time zone\n",
    "##\n",
    "import pytz\n",
    "def localize(data, datasource, dataset):\n",
    "    print \"Converting \" + dataset[\"name\"] + \" from \" + datasource[\"timezone\"] + \" to \" + dataset[\"timezone\"]\n",
    "    timezone = pytz.timezone(datasource[\"timezone\"])\n",
    "    data.index = data.index.tz_localize(\"Europe/London\").tz_convert(timezone)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Normalise (Candlesticks)\n",
    "##\n",
    "def normaliseCandlesticks(data):\n",
    "    X = data.values\n",
    "    Xmax = X.max(axis=1)[numpy.newaxis].T\n",
    "    Xmin = X.min(axis=1)[numpy.newaxis].T\n",
    "    scale = Xmax - Xmin\n",
    "    X = (X - Xmin) / scale\n",
    "    return pandas.DataFrame(numpy.hstack((X,scale / numpy.nanmax(scale))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Split (Train/Val/Test)\n",
    "##\n",
    "def split(data, train=.6, val=.2, test=.2):\n",
    "    idx = numpy.arange(0,len(data)) / float(len(data))\n",
    "    msk1 = data[idx<train]\n",
    "    msk2 = data[(idx>=train) & (idx<(train + val))]\n",
    "    msk3 = data[(idx>=(train+val))]\n",
    "    return [msk1, msk2, msk3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Merge New Data\n",
    "##\n",
    "\n",
    "def mergeNewData(data, datasource, dataset, SRC_path):\n",
    "    \n",
    "        ## Loop over any source files...\n",
    "        for infile in os.listdir(SRC_path):          \n",
    "\n",
    "            newData = loadRawData(datasource, dataset, SRC_path, infile)\n",
    "            if not newData is None:\n",
    "\n",
    "                ### RAW PIPELINE #############################################\n",
    "\n",
    "                newData = localize(newData, datasource, dataset)\n",
    "                \n",
    "                data = merge(newData, data)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "      \n",
    "\n",
    "                ##############################################################\n",
    "        \n",
    "        return data\n",
    "    \n",
    "\n",
    "def loadRawData(datasource, dataset, srcPath, infile):\n",
    "\n",
    "    if infile.lower().startswith(dataset[\"name\"].lower()):\n",
    "\n",
    "        print \"Adding \" + infile + \" to \" + dataset[\"name\"] + \" table\"\n",
    "\n",
    "        ## Load RAW data (assume CSV)\n",
    "        return pandas.read_csv(srcPath + infile, \n",
    "                                  index_col=datasource[\"index_col\"], \n",
    "                                  parse_dates=datasource[\"parse_dates\"], \n",
    "                                  dayfirst=datasource[\"dayfirst\"]\n",
    "                                 )        \n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Save data\n",
    "##\n",
    "def save_hdf(data, dataset, hdfStore):\n",
    "    hdfStore.put(dataset[\"name\"], data, format='table')\n",
    "    print \"Saved data to HDFStore: /\" + dataset[\"name\"]\n",
    "    return data\n",
    "\n",
    "def save_csv(data, filename):\n",
    "    data.to_csv( filename, mode=\"w\", header=False, index=False)\n",
    "    print \"Saved data to \" + filename\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.offline as py\n",
    "from plotly.tools import FigureFactory as FF\n",
    "#from plotly import tools as pt\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "plotly.offline.init_notebook_mode() # run at the start of every ipython notebook\n",
    "\n",
    "##\n",
    "## Visualise\n",
    "##\n",
    "def visualise(data, periods, count):\n",
    "    csticks = data.values[0:count:,:periods*4].ravel().reshape(-1,4)\n",
    "\n",
    "    fig = FF.create_candlestick(csticks[:,0], csticks[:,1], csticks[:,2], csticks[:,3])\n",
    "\n",
    "    py.iplot(fig, filename='jupyter/simple-candlestick', validate=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Set up the environment\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "CONFIG_FILE = \"datasets/config.json\"\n",
    "\n",
    "with open(CONFIG_FILE) as data_file:    \n",
    "    config = json.load(data_file)\n",
    "\n",
    "DS = config[\"datasources\"]\n",
    "FEATURES = config[\"features\"]\n",
    "CLASS = config[\"class\"]\n",
    "\n",
    "## Loop over datasources...\n",
    "\n",
    "for datasource in DS:\n",
    "    \n",
    "    DS_path = config[\"dataPath\"] + datasource[\"name\"] + \"/\"\n",
    "    SRC_path = DS_path + \"raw/\"\n",
    "        \n",
    "    # Create folder structure\n",
    "    if not os.path.exists(SRC_path):\n",
    "        os.makedirs(SRC_path)\n",
    "    \n",
    "    # Get HDFStore\n",
    "    hdfFile = DS_path + datasource[\"name\"] + \".hdf\"\n",
    "    hdfStore = pandas.HDFStore(hdfFile)\n",
    "    \n",
    "    for dataset in datasource[\"datasets\"]:\n",
    "        \n",
    "        # Load Dataframe from store\n",
    "        if dataset[\"name\"] in hdfStore:\n",
    "            storedData = hdfStore[dataset[\"name\"]]\n",
    "        else:\n",
    "            storedData = pandas.DataFrame()\n",
    "            \n",
    "        ### PIPELINE ###################\n",
    "        \n",
    "        ## Resample all to dataset sample unit (to introduce nans in all missing periods)\n",
    "        ## Resample feature units\n",
    "        ## Crop on time for feature\n",
    "        ## Resample class units\n",
    "        ## Crop on time for class\n",
    "        ## Convert to Feature Sets\n",
    "        ## Encode the class\n",
    "        ## Concat the two\n",
    "                        \n",
    "        storedData = mergeNewData(storedData, datasource, dataset, SRC_path)     \n",
    "        \n",
    "        data = cropDate(storedData, dataset[\"crop\"][\"start\"], dataset[\"crop\"][\"end\"])\n",
    "\n",
    "        ## Clean selected data set\n",
    "        \n",
    "        data = resample(data, dataset[\"sample_unit\"])\n",
    "        \n",
    "        featureData = resample(data, FEATURES[\"sample_unit\"]) \n",
    "        \n",
    "        featureData = cropTime(featureData, FEATURES[\"start_time\"], FEATURES[\"end_time\"])\n",
    "        \n",
    "        featureData = toFeatureSet(featureData, FEATURES[\"periods\"])\n",
    "        \n",
    "        featureData = normaliseCandlesticks(featureData)\n",
    "        \n",
    "        classData = resample(data, CLASS[\"sample_unit\"]) \n",
    "        \n",
    "        classData = cropTime(classData, CLASS[\"start_time\"], CLASS[\"end_time\"])\n",
    "        \n",
    "        classData = toFeatureSet(classData, CLASS[\"periods\"])\n",
    "        \n",
    "        classData = encode(classData, CLASS[\"encoding\"])   \n",
    "             \n",
    "        csvData = concat(featureData, classData)\n",
    "             \n",
    "        csvData = removeNaNs(csvData)\n",
    "        \n",
    "        csvData = shuffle(csvData)\n",
    "        \n",
    "        csvData_train, csvData_val, csvData_test = split(csvData, train=.6, val=.2, test=.2)\n",
    "        \n",
    "        visualise(csvData_train, FEATURES[\"periods\"], 5)\n",
    "       \n",
    "      \n",
    "        ########################################\n",
    "        \n",
    "        ## Save output\n",
    "        save_hdf(storedData, dataset, hdfStore)\n",
    "        save_csv(csvData, DS_path + dataset[\"name\"] + \".csv\")\n",
    "        save_csv(csvData_train, DS_path + dataset[\"name\"] + \"_train.csv\")\n",
    "        save_csv(csvData_val, DS_path + dataset[\"name\"] + \"_val.csv\") \n",
    "        save_csv(csvData_test, DS_path + dataset[\"name\"] + \"_test.csv\") \n",
    "\n",
    "    hdfStore.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
